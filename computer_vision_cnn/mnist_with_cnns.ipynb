{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will make a simple CNN and train fashion MNIST with it. Fashion MNIST is like number MNIST, instead of numbers it has got images of wearables. The classes in Fashion MNIST are\n",
    "\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "#This segment verifies the setup\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "IS_COLAB=0\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following in this section\n",
    "- Load mnist data from Keras with train_full and test sets\n",
    "- Create train set and valid set\n",
    "- Normalise the input data\n",
    "- Add channel dimension to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of full train and test sets (60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "item0 9\n",
      "Shape of train and valid sets (25000, 28, 28) (25000,) (5000, 28, 28) (5000,)\n",
      "Shape of features (25000, 28, 28, 1) (5000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "print (\"Shape of full train and test sets\", X_train_full.shape, y_train_full.shape, X_test.shape, y_test.shape)\n",
    "X_train, X_valid = X_train_full[:-35000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-35000], y_train_full[-5000:]\n",
    "\n",
    "plt.imshow(X_train_full[0], cmap=\"Greys\")\n",
    "print (\"item0\", y_train_full[0])\n",
    "print (\"Shape of train and valid sets\", X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "\n",
    "#Normalise the features set with mean and standard deviations\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "#Add an extra dimension for channels and compatibility to tools\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "print (\"Shape of features\", X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image is 28x28 and has a single channel\n",
    "- MaxPooling of 2 reduces spacial dimension by 2 keeping memory requirements in control\n",
    "- Increase the number of filters (doubling)as we go deeper to create combine lower level feature to higher level features\n",
    "- We flatten the data and feed to the dense layers in the end. Makes it into a single dimensional array that can be input to dense layers\n",
    "- There is 50% normalisation to reduce overfitting\n",
    "- And finally a 10 output dense layer with softmax to bring it down to 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.MaxPooling2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "#Create a function with which we can more easily add convolution layers\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "# def DefaultConv2D(filters, kernel_size, input_shape):\n",
    "#     return keras.layers.Conv2D(filters, kernel_size, input_shape,  kernel_size=3, \n",
    "#         activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]), # 28x28x64\n",
    "    keras.layers.MaxPooling2D(pool_size=2), #Same maxpooling applied regularly -> 14x14x64\n",
    "    DefaultConv2D(filters=128), # 14x14x128 \n",
    "    DefaultConv2D(filters=128), # 14x14x128\n",
    "    keras.layers.MaxPooling2D(pool_size=2), #7x7x128\n",
    "    DefaultConv2D(filters=256), #7x7x256\n",
    "    DefaultConv2D(filters=256), #7x7x256\n",
    "    keras.layers.MaxPooling2D(pool_size=2), # 3x3x256\n",
    "    DefaultConv2D(filters=256), # 3x3x256\n",
    "    DefaultConv2D(filters=256), # 3x3x256\n",
    "    keras.layers.MaxPooling2D(pool_size=2), # 1x1x256\n",
    "    keras.layers.Flatten(), #[256]\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 64)        3200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,331,850\n",
      "Trainable params: 2,331,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 289s 368ms/step - loss: 0.4681 - accuracy: 0.8408 - val_loss: 0.4637 - val_accuracy: 0.8322\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 288s 369ms/step - loss: 0.4096 - accuracy: 0.8610 - val_loss: 0.3990 - val_accuracy: 0.8652\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.4160 - accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 288s 368ms/step - loss: 0.3856 - accuracy: 0.8676 - val_loss: 0.3893 - val_accuracy: 0.8646\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 289s 369ms/step - loss: 0.3564 - accuracy: 0.8769 - val_loss: 0.3816 - val_accuracy: 0.8764\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.3994 - accuracy: 0.8716\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 290s 370ms/step - loss: 0.3317 - accuracy: 0.8854 - val_loss: 0.3932 - val_accuracy: 0.8802\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 300s 384ms/step - loss: 0.3178 - accuracy: 0.8946 - val_loss: 0.3875 - val_accuracy: 0.8650\n",
      "313/313 [==============================] - 28s 90ms/step - loss: 0.3875 - accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39938512444496155, 0.8715999722480774]\n"
     ]
    }
   ],
   "source": [
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[10:20] # pretend we have new images\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 7 3 4 1 2 4 8 0]\n"
     ]
    }
   ],
   "source": [
    "#Print actual data from test\n",
    "print (y_test[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 5 3 4 1 2 4 8 0]\n"
     ]
    }
   ],
   "source": [
    "#Print the output of the predicted set\n",
    "print (np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
